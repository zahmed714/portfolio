<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
  <title>Learning Blog | Zuhayr Ahmed</title>
</head>

<body class="blog-page">
  <header class="site-header">
    <div class="container">
      <nav class="navbar">
        <div class="navdiv">
          <div class="logo">
            <a href="index.html#home">ZA</a>
          </div>
          <ul class="nav-menu">
            <li><a href="index.html#about">About</a></li>
            <li><a href="index.html#projects">Projects</a></li>
            <li><a href="#posts">Blog Posts</a></li>
            <li><a href="index.html#certificates">Certificates</a></li>
            <li><a href="index.html#raspberry-videos">Videos</a></li>
            <li><a href="index.html#contact" class="cta">Contact</a></li>
          </ul>
          <div class="mobile-menu-toggle">
            <span></span>
            <span></span>
            <span></span>
          </div>
        </div>
      </nav>
    </div>
    <div class="header-gradient"></div>
  </header>

  <section class="blog-hero">
    <div class="container">
      <h1>Blog Posts</h1>
      <div class="blog-actions">
        <a href="#posts" class="btn">Jump to posts</a>
        <a href="index.html#projects" class="btn secondary-btn">Back to projects</a>
      </div>
    </div>
  </section>

  <section id="posts" class="blog-posts">
    <div class="container">
      <div class="blog-list">
        <article class="blog-card open" id="dec-30-2025">
          <div class="blog-card-header">
            <div>
              <div class="blog-meta">
                <span class="blog-date">Dec 30, 2025</span>
              </div>
              <h3 class="blog-card-title">AI Agent Development Environments</h3>
            </div>
            <button class="blog-toggle" data-toggle-post aria-expanded="true">Collapse</button>
          </div>

          <div class="blog-card-content">
            <div class="blog-card-copy">
              <p>I spent today exploring the various development environments for building AI agents, particularly
                within the <strong>Microsoft Azure</strong> ecosystem, where the choice of platform depends on the
                required balance between autonomy and ease of use. For business users or "citizen developers,"
                <strong>Copilot Studio</strong> provides a low-code, visual interface for quickly deploying agents to
                common channels like Slack or Microsoft 365, while the <strong>Microsoft 365 Agents SDK</strong> offers
                professional developers the flexibility to build more complex, self-hosted solutions.</p>
              <p>I also looked at the <strong>Foundry Agent Service</strong>, a managed Azure service that provides
                deeper model choices and enterprise-grade security for scalable agentic applications, as well as
                <strong>AutoGen</strong>, an open-source framework ideal for rapidly researching and experimenting with
                multi-agent coordination. Understanding these environments is essential because they provide the
                infrastructure needed for agents to perform knowledge integration, task automation, and autonomous
                decision-making securely within a business workflow.</p>
            </div>
          </div>
        </article>

        <article class="blog-card" id="dec-29-2025">
          <div class="blog-card-header">
            <div>
              <div class="blog-meta">
                <span class="blog-date">Dec 29, 2025</span>
              </div>
              <h3 class="blog-card-title">Random Forests & Bootstrapping</h3>
            </div>
            <button class="blog-toggle" data-toggle-post aria-expanded="true">Collapse</button>
          </div>

          <div class="blog-card-content">
            <div class="blog-card-copy">
              <p>I spent time today looking at <strong>ensemble techniques</strong> to see how combining multiple models
                can lead to much better predictions than just using one tree. I focused on <strong>bagging</strong> and
                <strong>boosting</strong>, which are different ways to group models together, like in a <strong>Random
                  Forest</strong>.
              </p>
              <p>I learned about <strong>bootstrapping</strong>, where you create random samples of data to train
                different versions of a model. This led to <strong>out-of-bag error</strong>, which is a method for
                testing the model using the data points that were left out of the training sample. It is a really
                efficient way to validate how accurate a model is without needing a completely separate test set.</p>
            </div>
            <div class="blog-card-media">
              <img src="images/Blogs/dec29-1.png" alt="Notes on Random Forests, bootstrapping, and out-of-bag error">
            </div>
          </div>
        </article>

        <article class="blog-card" id="dec-28-2025">
          <div class="blog-card-header">
            <div>
              <div class="blog-meta">
                <span class="blog-date">Dec 28, 2025</span>
              </div>
              <h3 class="blog-card-title">Ensemble Techniques: Bagging & Boosting</h3>
            </div>
            <button class="blog-toggle" data-toggle-post aria-expanded="true">Collapse</button>
          </div>

          <div class="blog-card-content">
            <div class="blog-card-copy">
              <p>I spent today exploring <strong>Ensemble Techniques</strong> to see how combining multiple models can
                lead to much more accurate results than using a single decision tree. I focused specifically on
                <strong>Bagging</strong> and <strong>Boosting</strong>, which are two different ways to build these
                "forests" of trees.
              </p>
              <p>I spent a good chunk of time on <strong>Bootstrapping</strong>, which involves creating multiple random
                samples from the original dataset to train different trees. This led into the concept of
                <strong>Out-of-bag (OOB) error</strong>, a clever way to validate a model while it is still being
                trained. Since a "bag" or bootstrap sample typically leaves out some data points, you can use those
                excluded points as a test set for that specific tree. It's an efficient method because it gives you a
                clear sense of the prediction error without needing a separate, dedicated validation set.
              </p>
            </div>
            <div class="blog-card-media">
              <img src="images/Blogs/dec28-1.png" alt="Notes on Ensemble Techniques, Bagging, Boosting, and OOB error">
            </div>
          </div>
        </article>

        <article class="blog-card" id="dec-27-2025">
          <div class="blog-card-header">
            <div>
              <div class="blog-meta">
                <span class="blog-date">Dec 27, 2025</span>
              </div>
              <h3 class="blog-card-title">Understanding Entropy in Decision Trees</h3>
            </div>
            <button class="blog-toggle" data-toggle-post aria-expanded="true">Collapse</button>
          </div>

          <div class="blog-card-content">
            <div class="blog-card-copy">
              <p>I spent today focusing on <strong>entropy</strong> and how it acts as the mathematical heart of a
                decision tree. In the context of machine learning, entropy is a way to quantify the "purity" or "signal
                strength" of a specific region of data. It describes how well-mixed or well-separated a distribution is:
                a perfectly uniform, "messy" distribution has an entropy of 1, while a perfectly separated category has
                an entropy of 0.</p>
            </div>
            <div class="blog-card-media">
              <img src="images/Blogs/dec27-1.gif" alt="Animated visualization of entropy in decision trees">
            </div>
          </div>
        </article>

        <article class="blog-card" id="dec-26-2025">
          <div class="blog-card-header">
            <div>
              <div class="blog-meta">
                <span class="blog-date">Dec 26, 2025</span>
              </div>
              <h3 class="blog-card-title">Decision Trees & Information Gain</h3>
            </div>
            <button class="blog-toggle" data-toggle-post aria-expanded="true">Collapse</button>
          </div>

          <div class="blog-card-content">
            <div class="blog-card-copy">
              <p>I spent time working on <strong>Decision Trees</strong> through the HarvardX CS109xa course to
                understand how models make predictions by following a flowchart of logical questions. I learned about
                using <strong>entropy</strong> to measure the randomness or disorder in a dataset and calculating
                <strong>information gain</strong> to decide which features provide the best splits.
              </p>
            </div>
            <div class="blog-card-media">
              <img src="images/Blogs/dec26-1.jpg" alt="Notes on Decision Trees, entropy, and information gain">
            </div>
          </div>
        </article>

        <article class="blog-card" id="dec-25-2025">
          <div class="blog-card-header">
            <div>
              <div class="blog-meta">
                <span class="blog-date">Dec 25, 2025</span>
              </div>
              <h3 class="blog-card-title">Evaluating Binary Classifiers</h3>
            </div>
            <button class="blog-toggle" data-toggle-post aria-expanded="true">Collapse</button>
          </div>

          <div class="blog-card-content">
            <div class="blog-card-copy">
              <p>I spent some time really breaking down how to evaluate a binary classifier using more than just a basic
                accuracy score by looking at the conditional probabilities of <strong>sensitivity</strong> and
                <strong>specificity</strong>. Sensitivity is the probability of a positive test given that the disease
                is actually present (P(Test + | Disease)), while specificity is the probability of a negative test given
                the person is healthy (P(Test - | Healthy)).
              </p>
              <p>I also looked into how <strong>prevalence</strong>, or the overall probability of the disease in the
                population (P(Disease)), influences these results and the overall predictive power of the model. It was
                interesting to see the mathematical trade-off involved; if you make a model ultra-sensitive to ensure
                you don't miss a single positive case, you almost always end up with a higher false-positive rate, which
                drags down your specificity. This forced me to think more about how we define the "Gold Standard" or
                ground truth versus the predictions our model actually spits out.</p>
            </div>
            <div class="blog-card-media media-grid">
              <img src="images/Blogs/dec25-1.png" alt="Notes on sensitivity and specificity for binary classifiers">
              <img src="images/Blogs/dec25-2.png" alt="Notes on prevalence and its effect on predictive power">
            </div>
          </div>
        </article>

        <article class="blog-card" id="dec-24-2025">
          <div class="blog-card-header">
            <div>
              <div class="blog-meta">
                <span class="blog-date">Dec 24, 2025</span>
              </div>
              <h3 class="blog-card-title">NumPy, Pandas, and Matplotlib Pipeline</h3>
            </div>
            <button class="blog-toggle" data-toggle-post aria-expanded="true">Collapse</button>
          </div>

          <div class="blog-card-content">
            <div class="blog-card-copy">
              <p>Today I moved deeper into the Python data science stack by working with <strong>NumPy</strong>,
                <strong>Pandas</strong>, and <strong>Matplotlib</strong>. Instead of just basic array creation, I
                focused on high-performance vectorization and broadcasting in NumPy to handle complex mathematical
                operations without slow loops.
              </p>
              <p>I also spent time in Pandas doing more advanced data manipulation, specifically focusing on
                multi-indexing and complex joins to clean up messy datasets for analysis. To wrap it all up, I used
                Matplotlib to build more than just simple line graphs, working on subplots and customized styling to
                better visualize how data distributions and correlations actually look. Getting these three libraries to
                work together really highlighted how a proper pipeline moves from raw data to a finished, visual
                insight.</p>
            </div>
            <div class="blog-card-media media-grid">
              <img src="images/Blogs/dec24-1.png" alt="Notebook showing NumPy vectorization and broadcasting examples">
              <img src="images/Blogs/dec24-2.png"
                alt="Pandas and Matplotlib workflow with multi-indexing and styled subplots">
            </div>
          </div>
        </article>

        <article class="blog-card" id="dec-23-2025">
          <div class="blog-card-header">
            <div>
              <div class="blog-meta">
                <span class="blog-date">Dec 23, 2025</span>
              </div>
              <h3 class="blog-card-title">Hands-on with Data Science Tools</h3>
            </div>
            <button class="blog-toggle" data-toggle-post aria-expanded="true">Collapse</button>
          </div>

          <div class="blog-card-content">
            <div class="blog-card-copy">
              <p>I spent some time today diving into the practical side of data science by playing around with some of
                its core tools. I used <strong>SPSS</strong> for data management and <strong>JASP</strong> to get a
                better feel for statistical inference and hypothesis testing. It helped clarify how we handle different
                data types, like categorical versus continuous variables.</p>
              <p>I also experimented with <strong>Tableau</strong> to see how to turn raw data into interactive
                dashboards, which makes it much easier to visualize descriptive stats like the mean, median, and mode.
                Getting hands-on with these programs really helped me understand the bigger picture of how to analyze
                trends and correlations without confusing them for causation.</p>
            </div>
            <div class="blog-card-media media-grid">
              <img src="images/Blogs/dec23-1.png" alt="Screens showing SPSS data management workflow">
              <img src="images/Blogs/dec23-2.png" alt="JASP interface for statistical inference and hypothesis testing">
              <img src="images/Blogs/dec23-3.png" alt="Tableau dashboard visualizing descriptive statistics">
            </div>
          </div>
        </article>

        <article class="blog-card" id="dec-22-2025">
          <div class="blog-card-header">
            <div>
              <div class="blog-meta">
                <span class="blog-date">Dec 22, 2025</span>
              </div>
              <h3 class="blog-card-title">Neural Networks, CNNs, and RNNs</h3>
            </div>
            <button class="blog-toggle" data-toggle-post aria-expanded="true">Collapse</button>
          </div>

          <div class="blog-card-content">
            <div class="blog-card-copy">
              <p>I spent today figuring out how AI actually "thinks" using <strong>Neural Networks</strong>. I learned
                how these models are trained using <strong>gradient descent</strong> to minimize loss and how
                <strong>backpropagation</strong> keeps the whole system improving.
              </p>
              <p>The coolest part was <strong>Computer Vision</strong>. <strong>Convolutional Neural Networks
                  (CNNs)</strong> process images with filters to highlight edges and shapes. I also learned about
                <strong>RNNs</strong>, which help AI remember what just happened, like when captioning a video or
                translating a sentence.
              </p>
            </div>
            <div class="blog-card-media media-grid">
              <img src="images/Blogs/dec22-1.png" alt="Notes on neural networks and gradient descent">
              <img src="images/Blogs/dec22-2.png" alt="Diagram explaining backpropagation and loss minimization">
              <img src="images/Blogs/dec22-3.png" alt="CNN filters highlighting edges and shapes in images">
              <img src="images/Blogs/dec22-4.png"
                alt="RNN notes covering sequences for video captioning and translation">
            </div>
          </div>
        </article>

        <article class="blog-card" id="dec-21-2025">
          <div class="blog-card-header">
            <div>
              <div class="blog-meta">
                <span class="blog-date">Dec 21, 2025</span>
              </div>
              <h3 class="blog-card-title">Uncertainty in AI</h3>
            </div>
            <button class="blog-toggle" data-toggle-post aria-expanded="true">Collapse</button>
          </div>

          <div class="blog-card-content">
            <div class="blog-card-copy">
              <p>Today I spent my time into learning how AI handles the "messiness" of the real world. I moved past
                simple logic into <strong>Uncertainty</strong>, using <strong>Bayes' Rule</strong> and <strong>Bayesian
                  Networks</strong> to calculate probabilities when information is incomplete. I also explored
                <strong>Optimization</strong>, learning how algorithms like <strong>Simulated Annealing</strong> and
                <strong>Hill Climbing</strong> find the best solutions in massive search spaces. Finally, I started on
                <strong>Machine Learning</strong>, covering everything from basic classification
                (<strong>Perceptrons</strong> and <strong>SVMs</strong>) to <strong>Reinforcement Learning</strong>,
                where an agent learns through trial and error.
              </p>
            </div>
            <div class="blog-card-media media-grid">
              <img src="images/Blogs/dec21-1.png"
                alt="Notes on handling uncertainty in AI with Bayes' Rule and Bayesian Networks">
              <img src="images/Blogs/dec21-2.png"
                alt="Notes on optimization methods like simulated annealing and hill climbing">
              <img src="images/Blogs/dec21-3.png"
                alt="Notes on machine learning, perceptrons, SVMs, and reinforcement learning">
            </div>
          </div>
        </article>

        <article class="blog-card" id="dec-20-2025">
          <div class="blog-card-header">
            <div>
              <div class="blog-meta">
                <span class="blog-date">Dec 20, 2025</span>
              </div>
              <h3 class="blog-card-title">Search &amp; Knowledge in AI</h3>
            </div>
            <button class="blog-toggle" data-toggle-post aria-expanded="true">Collapse</button>
          </div>

          <div class="blog-card-content">
            <div class="blog-card-copy">
              <p>Today, I dove into the core mechanics of how AI makes decisions. I explored <strong>Search
                  algorithms</strong>, moving from basic pathfinding (BFS/DFS) to optimized search like
                <strong>A*</strong> and game-playing logic with <strong>Minimax</strong>. I also covered
                <strong>Knowledge representation</strong>, learning how agents use propositional and first-order logic
                to "reason" and draw new conclusions from data.
              </p>
            </div>
            <div class="blog-card-media">
              <img src="images/Blogs/dec20-1.png"
                alt="Notes from Dec 20, 2025 on search algorithms and knowledge representation">
            </div>
          </div>
        </article>
      </div>
    </div>
  </section>

  <script>
    // Mobile menu toggle
    const mobileToggle = document.querySelector('.mobile-menu-toggle');
    const navMenu = document.querySelector('.nav-menu');
    const header = document.querySelector('.site-header');

    if (mobileToggle && navMenu) {
      mobileToggle.addEventListener('click', () => {
        mobileToggle.classList.toggle('active');
        navMenu.classList.toggle('active');
      });

      navMenu.addEventListener('click', (e) => {
        if (e.target.tagName === 'A') {
          mobileToggle.classList.remove('active');
          navMenu.classList.remove('active');
        }
      });
    }

    // Header scroll effect
    window.addEventListener('scroll', () => {
      if (window.scrollY > 50) {
        header.classList.add('scrolled');
      } else {
        header.classList.remove('scrolled');
      }
    });

    // Smooth scrolling for in-page links
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        const target = document.querySelector(this.getAttribute('href'));
        if (target) {
          e.preventDefault();
          const headerHeight = header ? header.offsetHeight : 0;
          const targetPosition = target.offsetTop - headerHeight;

          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });
        }
      });
    });

    // Blog card toggle
    document.querySelectorAll('[data-toggle-post]').forEach(button => {
      const card = button.closest('.blog-card');
      const isOpen = card.classList.contains('open');

      button.textContent = isOpen ? 'Collapse' : 'Expand';
      button.setAttribute('aria-expanded', isOpen);

      button.addEventListener('click', () => {
        const open = card.classList.toggle('open');
        button.textContent = open ? 'Collapse' : 'Expand';
        button.setAttribute('aria-expanded', open);
      });
    });
  </script>
</body>

</html>